# coding=utf-8
# Copyleft 2019 project LXRT.

import json
import os
import pickle

import numpy as np
import torch
from torch.utils.data import Dataset

from param import args
from utils import load_obj_tsv

# Load part of the dataset for fast checking.
# Notice that here is the number of images instead of the number of data,
# which means all related data to the images would be used.
TINY_IMG_NUM = 512
FAST_IMG_NUM = 5000

# The path to data and image features.
VQA_DATA_ROOT = '/data2/xinyi_wang/lxmert/data/vqa/'
MSCOCO_IMGFEAT_ROOT = '/data2/xinyi_wang/lxmert/data/mscoco_imgfeat/'
LABELS = ['NOT_HAL', 'HAL']


class HalDataset:
    """
    A caption data example in json file:
        {"image_id": 352538, "caption": "a brown purse is sitting on a green chair."}
    """
    def __init__(self, splits: str):
        self.name = splits
        self.splits = splits.split(',')

        # Loading datasets
        self.data = []
        for split in self.splits:
            with open('/data2/xinyi_wang/ImageCaptioning/myCOCO/{}.jsonl'.format(split)) as rf:
                for row in rf:
                    d = json.loads(row.strip())
                    d['label'] = 0
                    self.data.append(d)
            with open('/data2/xinyi_wang/ImageCaptioning/myCOCO/{}_hal.jsonl'.format(split)) as rf:
                for row in rf:
                    d = json.loads(row.strip())
                    d['label'] = 1
                    self.data.append(d)
        print("Load %d data from split(s) %s." % (len(self.data), self.name))

    def __len__(self):
        return len(self.data)


"""
An example in obj36 tsv:
FIELDNAMES = ["img_id", "img_h", "img_w", "objects_id", "objects_conf",
              "attrs_id", "attrs_conf", "num_boxes", "boxes", "features"]
FIELDNAMES would be keys in the dict returned by load_obj_tsv.
"""
class HalTorchDataset(Dataset):
    def __init__(self, dataset: HalDataset):
        super().__init__()
        self.raw_dataset = dataset

        if args.tiny:
            topk = TINY_IMG_NUM
        elif args.fast:
            topk = FAST_IMG_NUM
        else:
            topk = None

        # Loading detection features to img_data
        img_data = []
        for split in ['train2014', 'val2014']:
            # Minival is 5K images in MS COCO, which is used in evaluating VQA/LXMERT-pre-training.
            # It is saved as the top 5K features in val2014_***.tsv
            load_topk = 5000 if (split == 'minival' and topk is None) else topk
            img_data.extend(load_obj_tsv(
                os.path.join(MSCOCO_IMGFEAT_ROOT, '%s_obj36.tsv' % (split)),
                topk=load_topk))

        # Convert img list to dict
        self.imgid2img = {}
        for img_datum in img_data:
            imid = int(img_datum['img_id'].strip().split('_')[-1])
            self.imgid2img[imid] = img_datum

        # Only kept the data with loaded image features
        self.data = []
        for datum in self.raw_dataset.data:
            if datum['image_id'] in self.imgid2img:
                self.data.append(datum)
        print("Use %d data in torch dataset" % (len(self.data)))
        print()

    def __len__(self):
        return len(self.data)

    def __getitem__(self, item: int):
        datum = self.data[item]

        img_id = datum['image_id']
        cap = datum['caption']

        # Get image info
        img_info = self.imgid2img[img_id]
        obj_num = img_info['num_boxes']
        feats = img_info['features'].copy()
        boxes = img_info['boxes'].copy()
        assert obj_num == len(boxes) == len(feats)

        # Normalize the boxes (to 0 ~ 1)
        img_h, img_w = img_info['img_h'], img_info['img_w']
        boxes = boxes.copy()
        boxes[:, (0, 2)] /= img_w
        boxes[:, (1, 3)] /= img_h
        np.testing.assert_array_less(boxes, 1+1e-5)
        np.testing.assert_array_less(-boxes, 0+1e-5)

        # Provide label (target)
        if 'label' in datum:
            target = datum['label']
            return feats, boxes, cap, target
        else:
            return feats, boxes, cap

